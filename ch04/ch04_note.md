# ch04 note

## 学习
从训练数据中自动获取最优权重参数的过程

- 为了使神经网络能进行学习，将导入损失函数这一指标。
- 学习的目的就是以该损失函数为基准，找出能使它的值达到最小的权重参数。
- 为了找出尽可能小损失函数的值，将利用函数斜率的梯度法。

## 从数据中学习
神经网络的特征：可以从数据中学习

由数据自动决定权重参数的值

对于线性可分问题，根据“感知机收敛定理”，通过有限次数的学习，线性可分问题是可解的。
但是，非线性可分问题则无法通过（自动）学习来解决。

机器学习的方法极力避免人为介入，尝试从收集到的数据中发现答案（模式）。

神经网络或深度学习比以往的机器学习方法更能避免人为介入

先从图像中提取**特征量**，再用机器学习技术学习这些特征量的模式。
这里所说的“特征量”是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。

<p style="color: red; font-weight: bold;">将图像转换为向量时使用的特征量仍是由人设计的</p>

- 对于不同的问题，必须使用合适的特征量（必须设计专门的特征量），才能得到好的结果。
- 即使使用特征量和机器学习的方法，也需要针对不同的问题人工考虑合适的特征量。

神经网络的优点是对所有的问题都可以用同样的流程来解决

## 训练数据和测试数据
- 使用训练数据进行学习，寻找最优的参数。
- 使用测试数据评价训练得到的模型的实际能力

我们追求的是模型的泛化能力。为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。

泛化能力：处理未被观察过的数据（不包含在训练数据中的数据）的能力

<p style="color: red; font-weight: bold;">获得泛化能力是机器学习的最终目标</p>

仅仅用一个数据集去学习和评价参数，是无法进行正确评价的。
这样会导致可以顺利地处理某个数据集，但无法处理其他数据集的情况。

只对某个数据集过度拟合的状态称为**过拟合**

## 损失函数
神经网络以某个指标为线索寻找最优权重参数。神经网络的学习中所用的指标称为**损失函数**。
这个损失函数可以使用任意函数，但一般用<span style="color: deepskyblue; font-weight: bold;">均方误差</span>和<span style="color: deepskyblue; font-weight: bold;">交叉熵误差</span>等。

将正确解标签表示为1，其他标签表示为0的表示方法成为**one-hot表示**

### 均方误差
$$ E = \frac{1}{2} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$
其中，$y_i$表示神经网络的输出，$\hat{y}_i$表示监督数据（正确解标签），$n$表示数据的维数。

$$ E = \frac{1}{2} \sum_{k} (y_k - t_k)^2 $$
其中，$y_k$表示神经网络的输出，$t_k$表示监督数据（正确解标签），$k$表示数据的维数。

### 交叉熵误差
$$ E = - \sum_{k} t_k \log(y_k) $$
$$ E = - \sum_{i=1}^{n} \sum_{j=1}^{m} \hat{y}_{ij} \log(y_{ij}) $$
$$ E = - \frac{1}{N} \sum_{n} \sum_{k} t_{nk} \log(y_{nk})$$

交叉熵误差的值是由正确解标签所对应的输出结果决定的

## mini-batch学习
我们从全部数据中选出一部分，作为全部数据的“近似”。
神经网络的学习也是从训练数据中选出一批数据（称为mini-batch，小批量），然后对每个mini-batch进行学习。

## 为何要设定损失函数

对权重参数的损失函数求导，表示的是“如果稍微改变这个权重参数的值，损失函数的值会如何变化”。

之所以不能用识别精度作为指标，是因为这样以来绝大多数地方的导数都是0，导数参数无法更新。

<p style="color: red; font-weight: bold;">为什么用识别精度作为指标时，参数的导数在绝大多数地方都会变成0呢？</p>

识别精度对微小的参数变化基本上没有什么反应，即使有反应，它的值也是不连续地、突然地变化。

sigmoid函数的导数在任何地方都不为0，这对神经网络的学习非常重要。
得益于这个斜率不会为0的性质，神经网络的学习得以正确地进行。

## 梯度（gradient）
由全部变量的偏导数汇总而成的向量称为梯度

负梯度方向是梯度法中变量的更新方向

<p style="color: red; font-weight: bold;">梯度指示的方向是各点处的函数值减小最多的方向</p>

高等数学告诉我们，方向导数 = $ cos(\theta) \times$ 梯度（$ \theta $ 是方向导数的方向与梯度方向的夹角）。
因此，所有的下降方向中，梯度方向下降最多。 

- 梯度表示的是各点处的函数值减小最多的方向
- 无法保证梯度所指的方向就是函数的最小值或者真正应该前进的方向
- 在复杂的函数中，梯度指示的方向基本上都不是函数值最小处
- 虽然梯度的方向并不一定指向最小值，但沿着它的方向能够最大限度地减小函数的值
- 在寻找函数的最小值（或者尽可能小的值）的位置的任务中，要以梯度的信息为线索，决定前进的方向

<span style="color: red; font-weight: bold;">梯度下降中，为什么梯度的方向是最陡峭的呢？</span>

## 梯度法（gradient method）
- 通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法
- 通过不断地沿梯度方向前进，逐渐减少函数值的过程

梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。

## 学习率（learning rate）
$$ x_0 = x_0 - \eta \frac{\partial f}{\partial x_0} $$
$$ x_1 = x_1 - \eta \frac{\partial f}{\partial x_1} $$
$ \eta $ 表示更新量，在神经网络的学习中，称为学习率

学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数。

- 学习率需要事先确定为某个值，比如0.01或0.001。
- 一般而言，这个值过大或过小，都无法抵达一个“好的位置”。
- 在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了。

## 基于测试数据的评价
**过拟合**：虽然训练数据中的数字图像能被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象。

神经网络学习的最初目标是掌握泛化能力，因此，要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据。